{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import base64\n",
    "import requests\n",
    "import hashlib \n",
    "import time\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "import subprocess as sp\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from github import Github\n",
    "from pyDataverse.api import Api, NativeApi\n",
    "from pyDataverse.models import Datafile, Dataset\n",
    "\n",
    "from config import DV_ALIAS, BASE_URL, API_TOKEN, REPO, GITHUB_TOKEN, PARSABLE_EXTENSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(content: str)->list:\n",
    "    matches = re.findall(r\"(http[^\\s'\\\"\\\\]+)\", content)\n",
    "    pattern = re.compile(r\"([^/\\w]+)$\")\n",
    "    return [pattern.sub(\"\", match) for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_github_content(content: str) -> str:\n",
    "    return base64.b64decode(content).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_id(repo_name):\n",
    "    return hashlib.md5(repo_name.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_default_dataset(data, repo_name):\n",
    "    ds_id = make_dataset_id(repo_name)    \n",
    "    data[ds_id] = {'metadata': make_dataset_metadata(repo_name)}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_metadata(repo_name):\n",
    "    metadata = {}\n",
    "    metadata['termsOfAccess'] = ''\n",
    "    metadata['title'] = 'Automatic uploads from {} github repository'.format(repo_name)\n",
    "    metadata['subtitle'] = ''\n",
    "    metadata['author'] = [{\"authorName\": repo_name,\"authorAffiliation\": \"Coronawhy\"}]\n",
    "    metadata['dsDescription'] = [{'dsDescriptionValue': ''}]\n",
    "    metadata['subject'] = ['Medicine, Health and Life Sciences']\n",
    "    metadata['datasetContact'] = [{'datasetContactName': 'https://github.com/{}'.format(repo_name),'datasetContactEmail': ''}]\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file_metadata(repo_name, file, url):\n",
    "    metadata = {}\n",
    "\n",
    "    metadata['description'] = file\n",
    "    metadata['filename'] = url\n",
    "    metadata['datafile_id'] = hashlib.md5(url.encode(\"utf-8\"))\n",
    "    metadata['dataset_id'] = hashlib.md5(repo_name.encode(\"utf-8\"))\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(api, ds, dv_alias, mapping_dsid2pid, ds_id, base_url):\n",
    "    try:\n",
    "        resp = api.create_dataset(dv_alias, ds.json())\n",
    "        pid = resp.json()['data']['persistentId']\n",
    "    except:\n",
    "        print(resp.content)\n",
    "        return resp, mapping_dsid2pid\n",
    "    \n",
    "    mapping_dsid2pid[ds_id] = pid\n",
    "    time.sleep(1)\n",
    "    print('{0}/dataset.xhtml?persistentId={1}&version=DRAFT'.format(base_url,\n",
    "                                                                    pid))\n",
    "    return resp, mapping_dsid2pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation adapted from http://guides.dataverse.org/en/latest/api/native-api.html#id62\n",
    "def upload_datafile(server, api_key, p_id, repo_name, filename, repo_file, url):\n",
    "    dataverse_server = server\n",
    "    api_key = api_key\n",
    "    persistentId = p_id\n",
    "\n",
    "\n",
    "    files = {'file': (url.split('/')[-1], open(filename, 'rb'))}\n",
    "\n",
    "    params = dict(description=repo_file,\n",
    "                categories=[repo_name.split('/')[1]])\n",
    "\n",
    "    params_as_json_string = json.dumps(params)\n",
    "\n",
    "    payload = dict(jsonData=params_as_json_string)\n",
    "\n",
    "    url_persistent_id = '%s/api/datasets/:persistentId/add?persistentId=%s&key=%s' % (dataverse_server, persistentId, api_key)\n",
    "\n",
    "    print('-' * 40)\n",
    "    print('making request')\n",
    "    r = requests.post(url_persistent_id, data=payload, files=files)\n",
    "\n",
    "    print('-' * 40)\n",
    "    try:\n",
    "        print(r.json())\n",
    "    except:\n",
    "        print(r.content)\n",
    "    print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(GITHUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find urls in selected file extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 URLs\n",
      "Found 2 URLs\n",
      "Found 3 URLs\n",
      "Found 4 URLs\n",
      "Found 5 URLs\n",
      "Found 6 URLs\n",
      "Found 7 URLs\n",
      "Found 8 URLs\n",
      "Found 9 URLs\n",
      "Found 10 URLs\n",
      "Found 11 URLs\n",
      "Found 12 URLs\n",
      "Found 13 URLs\n",
      "Found 14 URLs\n",
      "Found 15 URLs\n",
      "Found 16 URLs\n",
      "Found 17 URLs\n",
      "Found 18 URLs\n"
     ]
    }
   ],
   "source": [
    "repo = g.get_repo(REPO)\n",
    "contents = repo.get_contents(\"\")\n",
    "urls_found = {}\n",
    "while contents:\n",
    "    file_content = contents.pop(0)\n",
    "    if file_content.type == \"dir\":\n",
    "        contents.extend(repo.get_contents(file_content.path))\n",
    "        continue\n",
    "        \n",
    "    if len(PARSABLE_EXTENSIONS) == 0 or file_content.name.split('.')[-1] in PARSABLE_EXTENSIONS:\n",
    "        urls = extract_urls(decode_github_content(file_content.content))\n",
    "        if len(urls) > 0:\n",
    "            urls_found[file_content.path] = extract_urls(decode_github_content(file_content.content))\n",
    "\n",
    "print('Found {} URLs'.format(len(urls_found)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset in dataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_api = NativeApi(BASE_URL, API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_id = str(int(make_dataset_id(REPO).hexdigest(), 16))[:6] ## turn the md5 string into a 6 digits integer\n",
    "metadata = make_dataset_metadata(REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with pid 'doi:10.5072/FK2/YNJVRV' created.\n",
      "http://datasets.coronawhy.org/dataset.xhtml?persistentId=doi:10.5072/FK2/YNJVRV&version=DRAFT\n"
     ]
    }
   ],
   "source": [
    "mapping_dsid2pid = {}\n",
    "ds = Dataset()\n",
    "ds.set(metadata)\n",
    "ds.displayName=metadata['title']\n",
    "resp, mapping_dsid2pid = create_dataset(native_api, ds, DV_ALIAS, mapping_dsid2pid, ds_id, BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading files for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthieupons/miniconda3/envs/coronawhy-github-dataset-finder/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- uploading the following dataset https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv\n",
      "----------------------------------------\n",
      "making request\n",
      "----------------------------------------\n",
      "{'status': 'OK', 'data': {'files': [{'description': 'task_geo/data_sources/mobility/mobility_connector.py', 'label': 'Global_Mobility_Report.csv', 'restricted': False, 'version': 1, 'datasetVersionId': 53, 'categories': ['task-geo'], 'dataFile': {'id': 194, 'persistentId': '', 'pidURL': '', 'filename': 'Global_Mobility_Report.csv', 'contentType': 'text/csv', 'filesize': 19452980, 'description': 'task_geo/data_sources/mobility/mobility_connector.py', 'storageIdentifier': '172043838c6-9a77ad96b927', 'rootDataFileId': -1, 'md5': 'b6b10de4672a71e949474ad55038d20d', 'checksum': {'type': 'MD5', 'value': 'b6b10de4672a71e949474ad55038d20d'}, 'creationDate': '2020-05-11'}}]}}\n",
      "200\n",
      "- uploading the following dataset https://power.larc.nasa.gov/cgi-bin/v1/DataAccess.py\n",
      "----------------------------------------\n",
      "making request\n",
      "----------------------------------------\n",
      "{'status': 'OK', 'data': {'files': [{'description': 'task_geo/dataset_builders/nasa/nasa_connector.py', 'label': 'DataAccess.py', 'restricted': False, 'version': 1, 'datasetVersionId': 53, 'categories': ['task-geo'], 'dataFile': {'id': 195, 'persistentId': '', 'pidURL': '', 'filename': 'DataAccess.py', 'contentType': 'text/plain', 'filesize': 219, 'description': 'task_geo/dataset_builders/nasa/nasa_connector.py', 'storageIdentifier': '17204386132-deabac8ae831', 'rootDataFileId': -1, 'md5': 'dde017e124c402624f1a7f1cb2ff0bea', 'checksum': {'type': 'MD5', 'value': 'dde017e124c402624f1a7f1cb2ff0bea'}, 'creationDate': '2020-05-11'}}]}}\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthieupons/miniconda3/envs/coronawhy-github-dataset-finder/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- uploading the following dataset https://coronadatascraper.com/timeseries.csv\n",
      "----------------------------------------\n",
      "making request\n",
      "----------------------------------------\n",
      "{'status': 'OK', 'data': {'files': [{'description': 'task_geo/data_sources/covid/cds/cds_connector.py', 'label': 'timeseries.csv', 'restricted': False, 'version': 1, 'datasetVersionId': 53, 'categories': ['task-geo'], 'dataFile': {'id': 196, 'persistentId': '', 'pidURL': '', 'filename': 'timeseries.csv', 'contentType': 'text/csv', 'filesize': 48840353, 'description': 'task_geo/data_sources/covid/cds/cds_connector.py', 'storageIdentifier': '1720438b805-4f05960c509c', 'rootDataFileId': -1, 'md5': 'e2c2ef606c1d7bb23b46b1404db97731', 'checksum': {'type': 'MD5', 'value': 'e2c2ef606c1d7bb23b46b1404db97731'}, 'creationDate': '2020-05-11'}}]}}\n",
      "200\n",
      "- uploading the following dataset https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\n",
      "----------------------------------------\n",
      "making request\n",
      "----------------------------------------\n",
      "{'status': 'OK', 'data': {'files': [{'description': 'task_geo/data_sources/covid/nyt/nyt.py', 'label': 'us-counties.csv', 'restricted': False, 'version': 1, 'datasetVersionId': 53, 'categories': ['task-geo'], 'dataFile': {'id': 197, 'persistentId': '', 'pidURL': '', 'filename': 'us-counties.csv', 'contentType': 'text/csv', 'filesize': 5089593, 'description': 'task_geo/data_sources/covid/nyt/nyt.py', 'storageIdentifier': '1720438d61f-e5790d9a4237', 'rootDataFileId': -1, 'md5': 'f4b8bcc5cc3d02ace46054e42e42a540', 'checksum': {'type': 'MD5', 'value': 'f4b8bcc5cc3d02ace46054e42e42a540'}, 'creationDate': '2020-05-11'}}]}}\n",
      "200\n",
      "- uploading the following dataset https://data.census.gov/\n",
      "----------------------------------------\n",
      "making request\n",
      "----------------------------------------\n",
      "{'status': 'ERROR', 'message': 'Failed to add file to dataset.'}\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "for file, urls in urls_found.items():\n",
    "    for url in urls:\n",
    "        try:\n",
    "            tmpfile = urllib.request.urlretrieve(url) # retrieve the csv in a temp file, if there is a problem with the URL it throws and we continue\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            filename = 'file://{}'.format(tmpfile[0])\n",
    "            # TODO: try gzipped datasets as well\n",
    "            pd.read_csv(filename) # try reading it as csv, if fails continue\n",
    "            metadata = make_file_metadata(REPO, file, url)\n",
    "            print('- uploading the following dataset {}'.format(url))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        upload_datafile(BASE_URL, API_TOKEN, mapping_dsid2pid[ds_id], REPO, tmpfile[0], file, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
